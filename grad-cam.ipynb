{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import (\n",
    "    VGG16, preprocess_input, decode_predictions)\n",
    "from keras.preprocessing import image\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.framework import ops\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import sys\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "\n",
    "def target_category_loss(x, category_index, nb_classes):\n",
    "    return tf.multiply(x, K.one_hot([category_index], nb_classes))\n",
    "\n",
    "def target_category_loss_output_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "def load_image(path):\n",
    "    img_path = sys.argv[1]\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def register_gradient():\n",
    "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
    "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
    "        def _GuidedBackProp(op, grad):\n",
    "            dtype = op.inputs[0].dtype\n",
    "            return grad * tf.cast(grad > 0., dtype) * \\\n",
    "                tf.cast(op.inputs[0] > 0., dtype)\n",
    "\n",
    "def compile_saliency_function(model, activation_layer='conv2d_12'):\n",
    "    input_img = model.input\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    layer_output = layer_dict[activation_layer].output\n",
    "    max_output = K.max(layer_output, axis=3)\n",
    "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
    "    return K.function([input_img, K.learning_phase()], [saliency])\n",
    "\n",
    "def modify_backprop(model, name):\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({'Relu': name}):\n",
    "\n",
    "        # get layers that have an activation\n",
    "        layer_dict = [layer for layer in model.layers[1:]\n",
    "                      if hasattr(layer, 'activation')]\n",
    "\n",
    "        # replace relu activation\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == keras.activations.relu:\n",
    "                layer.activation = tf.nn.relu\n",
    "\n",
    "        # re-instanciate a new model\n",
    "        new_model = load_model('C:/Users/Canpolat/Documents/Uni/masterthesis/voter_Entwurf/best_model_voter1.h5')\n",
    "    return new_model\n",
    "\n",
    "def deprocess_image(x):\n",
    "    '''\n",
    "    Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    '''\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def grad_cam(input_model, image, category_index, layer_name):\n",
    "    model = Sequential()\n",
    "    model.add(input_model)\n",
    "\n",
    "    nb_classes = 3\n",
    "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
    "    model.add(Lambda(target_layer,\n",
    "                     output_shape = target_category_loss_output_shape))\n",
    "\n",
    "    loss = K.sum(model.layers[-1].output)\n",
    "    \n",
    "    for l in model.layers[0].layers:\n",
    "        if(l.name == layer_name):\n",
    "            print('found layer')\n",
    "            conv_output = l[0].output\n",
    "            print(conv_output)\n",
    "    \n",
    "    #conv_output = [l for l in model.layers[0].layers if l.name is layer_name][0].output\n",
    "    grads = normalize(K.gradients(loss, conv_output)[0])\n",
    "    gradient_function = K.function([model.layers[0].input], [conv_output, grads])\n",
    "\n",
    "    output, grads_val = gradient_function([image])\n",
    "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "\n",
    "    weights = np.mean(grads_val, axis = (0, 1))\n",
    "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * output[:, :, i]\n",
    "\n",
    "    cam = cv2.resize(cam, (150, 150))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    heatmap = cam / np.max(cam)\n",
    "\n",
    "    #Return to BGR [0..255] from the preprocessed image\n",
    "    image = image[0, :]\n",
    "    image -= np.min(image)\n",
    "    image = np.minimum(image, 255)\n",
    "\n",
    "    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "    cam = np.float32(cam) + np.float32(image)\n",
    "    cam = 255 * cam / np.max(cam)\n",
    "    return np.uint8(cam), heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 148, 148, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 74, 74, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 74, 74, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 72, 72, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 34, 34, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 15, 15, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 150)               1881750   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 2,843,099\n",
      "Trainable params: 2,843,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[0. 1. 0.]]\n",
      "found layer\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Conv2D' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-09f03e786eeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mpredicted_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mcam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheatmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_cam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessed_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'conv2d_12'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m#cv2.imwrite(\"gradcam.jpg\", cam)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-d95fbe9ba5e8>\u001b[0m in \u001b[0;36mgrad_cam\u001b[1;34m(input_model, image, category_index, layer_name)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'found layer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mconv_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Conv2D' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "img_path = 'C:/Users/Canpolat/Desktop/a/a.JPG'\n",
    "preprocessed_input = image.load_img(img_path, target_size=(150, 150))\n",
    "model = load_model('C:/Users/Canpolat/Documents/Uni/masterthesis/voter_Entwurf/best_model_voter1.h5')\n",
    "model.summary()\n",
    "\n",
    "preprocessed_input = image.img_to_array(preprocessed_input)\n",
    "preprocessed_input = preprocessed_input.reshape((1,) + preprocessed_input.shape)\n",
    "\n",
    "predictions = model.predict(preprocessed_input)\n",
    "print(predictions)\n",
    "top_1 = np.argmax(predictions[0])\n",
    "#print('Predicted class:')\n",
    "#print(top_1)\n",
    "\n",
    "predicted_class = np.argmax(predictions)\n",
    "cam, heatmap = grad_cam(model, preprocessed_input, predicted_class, 'conv2d_12')\n",
    "#cv2.imwrite(\"gradcam.jpg\", cam)\n",
    "\n",
    "#register_gradient()\n",
    "#guided_model = modify_backprop(model, 'GuidedBackProp')\n",
    "#saliency_fn = compile_saliency_function(guided_model)\n",
    "#saliency = saliency_fn([preprocessed_input, 0])\n",
    "#gradcam = saliency[0] * heatmap[..., np.newaxis]\n",
    "#cv2.imwrite(\"guided_gradcam.jpg\", deprocess_image(gradcam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 148, 148, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 74, 74, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 74, 74, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 72, 72, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 34, 34, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 15, 15, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 150)               1881750   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 2,843,099\n",
      "Trainable params: 2,843,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001CA06B165C8>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001CA06B25B08>, <tensorflow.python.keras.layers.core.Dropout object at 0x000001CA037A6E48>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001CA037DAC88>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001CA0381CD88>, <tensorflow.python.keras.layers.core.Dropout object at 0x000001CA03825C88>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001CA03860E88>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001CA063A25C8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001CA063A3048>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001CA063E1888>, <tensorflow.python.keras.layers.core.Flatten object at 0x000001CA063A82C8>, <tensorflow.python.keras.layers.core.Dropout object at 0x000001CA063EC6C8>, <tensorflow.python.keras.layers.core.Dense object at 0x000001CA0642FDC8>, <tensorflow.python.keras.layers.core.Dense object at 0x000001CA0642FB88>]\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Canpolat\\Anaconda3\\envs\\masterarbeit\\lib\\site-packages\\ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def focal_loss_category(gamma=2.):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        return - tf.reduce_sum(y_true * ((1 - y_pred) ** gamma) * tf.log(y_pred), axis=1)\n",
    "\n",
    "    return focal_loss\n",
    "\n",
    "\n",
    "model = load_model('C:/Users/Canpolat/Documents/Uni/masterthesis/voter_Entwurf/best_model_voter1.h5')\n",
    "\n",
    "model.summary()\n",
    "print(model.layers)\n",
    "img_width = 150\n",
    "img_height = 150\n",
    "\n",
    "# img_path = 'C:/Users/AlphaCat/Desktop/models/gmle_cnn_keras_classifier_resnet50/tf_keras/555.jpg'\n",
    "directory = 'C:/Users/Canpolat/Desktop/b'\n",
    "\n",
    "#img_path = 'C:/Users/Canpolat/Desktop/a/a.JPG'\n",
    "#preprocessed_input = image.load_img(img_path, target_size=(150, 150))\n",
    "\n",
    "for img_name in os.listdir(directory):\n",
    "    img_path = os.path.join(directory, img_name)\n",
    "    img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    preds = model.predict(x)\n",
    "    class_idx = np.argmax(preds[0])\n",
    "    class_output = model.output[:, class_idx]\n",
    "    last_conv_layer = model.get_layer(\"conv2d_11\")\n",
    "    grads = K.gradients(class_output, last_conv_layer.output)[0]\n",
    "    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "    pooled_grads_value, conv_layer_output_value = iterate([x])\n",
    "    for i in range(len(pooled_grads_value)):\n",
    "        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "\n",
    "    heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "    new_img_name = img_name.split('.')[0] + '_cam_' + str(class_idx) + '.jpg'\n",
    "    cv2.imwrite(os.path.join(directory, new_img_name), superimposed_img)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
